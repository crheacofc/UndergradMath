{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Number 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that an iterative method giving $x_n \\to r$ has *order of convergence* $p$ if\n",
    "$$\n",
    "\\lim_{n \\to \\infty} \\frac{|e_{n+1}|}{|e_{n}|^p} \\to C\n",
    "$$\n",
    "for some finite constant $C$.\n",
    "Thus *quadratic convergence* is the case of order $p=2$.\n",
    "\n",
    "It can be shown (but not in this course) that when the secant method converges to a simple root $r$ (one with $f'(r) \\neq 0$), the order of convergence is $\\frac{1 + \\sqrt{5}}{2}, = \\phi \\approx 1.61$, the so-called Golden Ratio.\n",
    "\n",
    "Consider the ``double secant method'', in which each step is two successive steps of the secant method.\n",
    "- Explain why each step of this method is likely to have a time cost comparable to a single step of Newton's method.\n",
    "- Show that when this method converges to a simple root, the order of convergence is $\\phi^2$.\n",
    "\n",
    "Noting that $1 < \\phi < 2 < \\phi^2$, use this to illustrate that, for methods of super-linear convergence, the actual order of convergence is not very useful for comparing the speed of different methods.\n",
    "(The most useful speed distinction is just linear vs super-linear.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) Explain why each step of this method has a time cost comparable to a single step of Newton's method.\n",
    "In Newton's method we had to calculate two  function values, while in the second method we only had to calculate a single new function value per iteration. Hence if use the double secent method, then we would have to determine two new function values per iteration. Thus, its time cost is comparable to Newton's method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the double secent method converges to a simple root, we have \n",
    "$$\n",
    "\\lim_{n \\to \\infty} \\frac{|e_{n+1}|}{|e_{n}|^\\phi} \\to C\n",
    "$$\n",
    "And the previous step's error looks like \n",
    "$$\n",
    "\\lim_{n \\to \\infty} \\frac{|e_{n}|}{|e_{n-1}|^\\phi} \\to C\n",
    "$$\n",
    "\n",
    "So we have \n",
    "$$\n",
    "\\lim_{n \\to \\infty} |e_{n+1}| = C|e_{n}|^\\phi\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\lim_{n \\to \\infty} |e_{n}| = C|e_{n-1}|^\\phi\n",
    "$$\n",
    "Hence we have \n",
    "$$\\lim_{n \\to \\infty} |e_{n+1}| = C|e_{n}|^\\phi = C(C|e_{n-1}|^\\phi)^\\phi = C^{1+\\phi}|e_{n-1}|^{\\phi^2}$$\n",
    "\n",
    "Therefore, the order of convergence is $\\phi^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one thousand non-negative numbers are entered into software that uses IEEE64 floating point numbers and arithmetic and then added, what is the largest possible relative error in the result?\n",
    "Note that each intermediate result is computed as an IEEE64 machine number, so there can be rounding at every step.\n",
    "\n",
    "Express your result as the maximum number of decimal places that can be trusted in the computed value for the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: Induction Proof!  \n",
    "Claim: Adding n numbers and rounding will give a max error of n*u* where $u$.  \n",
    "Proof:  \n",
    "Base Case: n=2 (If n=1 then I wouldnt be adding any numbers...)  \n",
    "$\\tilde{x}=x(1+r)$ where $|r| \\leq u$ is the relative error.  \n",
    "$\\tilde{y}=y(1+s)$ where $|s| \\leq u$ is the relative error as well.\n",
    "Consider adding *x* and *y*,  \n",
    " $z=x+y \\rightarrow \\tilde{z}=\\tilde{x}+\\tilde{y} = z(1+t)$ where $|t| \\leq max(|r|,|s|)$    \n",
    "Rounding again gives $\\check{z} = fl(\\tilde{z})=z(1+v)$ where $|v| \\leq u$  \n",
    "Hence $\\check{z} = \\tilde{z}(1+v) = z(1+v)(1+t) = z(1+\\{t+v+tv\\})$  \n",
    "So the relative error is  $\\leq |t|+|v|+|tv| \\approx |t|+|v| \\leq 2u$  \n",
    "Therefore for *n=2* the max relative error $\\leq 2u$  \n",
    "\n",
    "Inductive case:  \n",
    "Assume that the max relative error for *n* added and rounded numbers is *nu*.  \n",
    "Consider $\\tilde{d} = d(1+c)$ where $|c|\\leq u$  \n",
    "Now adding *d* to *n* yields,    \n",
    "$e = n+d \\rightarrow \\tilde{e} = \\tilde{n}+\\tilde{d} = e(1+q) $ where $q \\leq max(|c|,|nu|)$  \n",
    "Rounding again gives $\\check{e} = \\tilde{e}(1+\\epsilon) = e(1+q)(1+\\epsilon) = e(1+\\{q+\\epsilon+q\\epsilon\\})$  \n",
    "Hence the relative error is $\\leq |q|+|\\epsilon|+|q\\epsilon| \\leq |q|+|\\epsilon|\\leq u+nu = (n+1)u $  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus if we did 1000 additions and subsequent rounding we would be left with an error of *1000u*.  \n",
    "$u=1.110223e+{-16} \\rightarrow 1000*u=1000*1.110223^{e-16}=1.e^{3}*1.110223e^{-16}=1.110223e^{-13}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustrate why computing the roots of the quadratic equation $ax^2 + bx + c = 0$ with the standard formula\n",
    "$$\n",
    "x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n",
    "$$\n",
    "can sometimes give poor accuracy when evaluated using machine arithmetic such as the IEEE64 floating-point arithmetic used in Python (and Matlab).\n",
    "\n",
    "Then describe a careful procedure for always getting accurate answers.\n",
    "\n",
    "State the procedure first with words and mathematical formulas, and then express it in pseudocode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Why can the standard quadratic formula sometimes give poor accuracy?\n",
    "The standard quadratic formula can give poor accuracy if b>>c. If this is the case $\\sqrt{b^2-4ac} \\approx \\sqrt{b^2} = b$ Which then leads $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\approx \\frac{-b \\pm b}{2a}$ which leads to a near zero answer when you take the plus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Example\n",
    "from math import sqrt\n",
    "a=2\n",
    "b=1e10\n",
    "c=.001\n",
    "x1 = (-b+sqrt(b**2-4*a*c))/(2*a)\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see how off the correct answer can be.  \n",
    "Lets first consider b>0.\n",
    "To rectify this issue, we can multiply the top and bottom of the equation by the conjugate which yields,\n",
    "$$\n",
    "x_1 = \\frac{-b + \\sqrt{b^2 - 4ac}}{2a} = \\frac{(-b + \\sqrt{b^2 - 4ac})}{2a} \\frac{(-b-\\sqrt{b^2 - 4ac})}{2a(-b-\\sqrt{b^2 - 4ac})} = \\frac{b^2-b^2 - 4ac}{2a(-b-\\sqrt{b^2 - 4ac})}= \\frac{-4ac}{2a(-b-\\sqrt{b^2 - 4ac})} = \\frac{-2c}{(-b-\\sqrt{b^2 - 4ac})}\n",
    "$$\n",
    "$x_2$ will be as normal. i.e. $x_2 = -\\frac{b+\\sqrt{b^2-4ac}}{2a} $  \n",
    "If b<0 the roots are as follows:\n",
    "$$x_1 = \\frac{2c}{-b+\\sqrt{b^2-4ac}} $$\n",
    "$$x_2 = \\frac{-b+\\sqrt{b^2-4ac}}{2a} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we just need a code to handle the fact that b may be much grater than a or c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is just a code for the solving of roots, not the full code. Also I am not going to consider the case b=0.\n",
    "#First if b is positive\n",
    "from math import sqrt\n",
    "def main(a,b,c):\n",
    "    if b**2-4*abs(a*c)>10^4: #meaining if you would lose 4 sig figs...\n",
    "        if b>0: \n",
    "            root1 = (-2*c)/(-b-sqrt(b**2-4*a*c))\n",
    "            root2 = -(b+sqrt(b**2-4*a*c))/(2*a)\n",
    "        if b<0:\n",
    "            root1 = (2*c)/(-b+sqrt(b**2-4*a*c))\n",
    "            root2 = (-b+sqrt(b**2-4*a*c))/(2*a)\n",
    "        return [root1,root2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1e-13, -5000000000.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A little example from before\n",
    "a=2\n",
    "b=1e10\n",
    "c=.001\n",
    "main(a,b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how instead of getting a zero root we get a very small number instead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Compute the LU factorization of\n",
    "$$\n",
    "A = \\left[ \\begin{array}{ccc} 4.0 & 2.0 & 1.0 \\\\ 9.0 & 3.0 & 1.0 \\\\ 25.0 & 5.0 & 1.0 \\end{array} \\right],\n",
    "$$\n",
    "using the naive Gaussian elimination algorithm.\n",
    "Compute each intermediate result with decimal arithmetic, not fractions (and thus with some rounding), giving at least four significant digits. Do not use the results of a computer code for this!\n",
    "\n",
    "(Note that you could check your work by matrix multiplication, and one way to do that is to compute the *residual* $R = A - LU$, which should be very close to zero. It is OK to use Python for that.)\n",
    "\n",
    "b) Use this $LU$ factorization to solve\n",
    "$$\n",
    "Ax = b = \\left[ \\begin{array}{c} 0.6931  \\\\ 1.0986 \\\\ 1.6094 \\end{array} \\right],\n",
    "$$\n",
    "using forward and backward substitution.\n",
    "\n",
    "c) Check the result by computing the *residual* $r = b - Ax$, which again should be very close to zero. Again it is OK to use Python for this checking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) A=$\\left[ \\begin{array}{ccc} 4.0 & 2.0 & 1.0 \\\\ 9.0 & 3.0 & 1.0 \\\\ 25.0 & 5.0 & 1.0 \\end{array} \\right]$ \n",
    "\n",
    "First we find $l_{2,1}$.  $l_{2,1}= \\frac{a_{2,1}}{a_{1,1}}=\\frac{9.0}{4.0} = 2.25 $ \n",
    "\n",
    "Now we have A=$\\left[ \\begin{array}{ccc} 4.0 & 2.0 & 1.0 \\\\ 0 & -1.5 & -1.25 \\\\ 25.0 & 5.0 & 1.0 \\end{array} \\right]$\n",
    "\n",
    "$l_{3,1}= \\frac{a_{3,1}}{a_{1,1}}=\\frac{25.}{4.} = 6.25 $\n",
    "\n",
    "Now we have A=$\\left[ \\begin{array}{ccc} 4.0 & 2.0 & 1.0 \\\\ 0 & -1.5 & -1.25 \\\\ 0 & -7.5 & -5.25 \\end{array} \\right]$\n",
    "\n",
    "$l_{3,2}= \\frac{a_{3,2}}{a_{2,2}}=\\frac{-7.5}{.75} = -10 $\n",
    "\n",
    "Now we have A=$\\left[ \\begin{array}{ccc} 4.0 & 2.0 & 1.0 \\\\ 0 & -1.5 & -1.25 \\\\ 0 & 0 & 1.0 \\end{array} \\right]$\n",
    "\n",
    "Note that $L$ is simply a lower triangular matrix with 1's on the diagonal and $l$ values filling in the lower slots. Hence we have, \n",
    "$$L =\\left[ \\begin{array}{ccc} 1.0 & 0.0 & 0.0 \\\\ 2.25 & 1.0 & 0.0 \\\\ 6.25 & 5 & 1.0 \\end{array} \\right]$$\n",
    "\n",
    "And $U$ is simply the final version of $A$ after gaussian elimination. Hence, \n",
    "\n",
    "$$U =\\left[ \\begin{array}{ccc} 4.0 & 2.0 & 1.0 \\\\ 0 & -1.5 & -1.25 \\\\ 0 & 0 & 1 \\end{array} \\right]  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[4.0,2.0,1.0],[9.0,3.0,1.0],[25.0,5.0,1.0]])\n",
    "L = np.array([[1.0,0.0,0.0],[2.25,1.0,0.0],[6.25,5.0,1.0]])\n",
    "U = np.array([[4.0,2.0,1.0],[0.0,-1.5,-1.25],[0.0,0.0,1.0]])\n",
    "A-np.dot(L,U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus L and U are correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) $$\n",
    "Ax = b = \\left[ \\begin{array}{c} 0.6931  \\\\ 1.0986 \\\\ 1.6094 \\end{array} \\right],\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we begin by solving $Lc=b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence,\n",
    "$$Lc =\\left[ \\begin{array}{ccc} 1.0 & 0.0 & 0.0 \\\\ 2.25 & 1.0 & 0.0 \\\\ 6.25 & 5 & 1.0 \\end{array} \\right] \\left[ \\begin{array}{c} c_1 \\\\ c_2 \\\\ c_3 \\end{array} \\right] = \\left[ \\begin{array}{c} 0.6931 \\\\ 1.0986 \\\\ 1.6094 \\end{array} \\right] =b$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have the equations, \n",
    "$$c_1 = 0.6931 \\\\ 2.25c_1 + c_2 = 1.0986 \\\\ 6.25c_1 + 5c_2 + c_3 = 1.6094$$\n",
    "\n",
    "Subbing $c_1$ into equation 2 yields $c_2 = 1.0986 - 2.25(0.6931) = -0.460875$.   \n",
    "Subbing both $c_1$ and $c_2$ into equation 3 yields $ c_3 = 1.6094  - 6.25(0.6931) - 5(-0.460875) = -.4181$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, \n",
    "$$c = \\left[ \\begin{array}{c} 0.6931 \\\\ -.460875 \\\\ -.4181 \\end{array} \\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6931,  1.0986,  1.6094])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Quick Check\n",
    "L = np.array([[1.0,0.0,0.0],[2.25,1.0,0.0],[6.25,5.0,1.0]])\n",
    "C = np.array([.6931,-.460875,-.4181])\n",
    "np.dot(L,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we solve $Ux=c$ \n",
    "$$Ux =\\left[ \\begin{array}{ccc} 4.0 & 2.0 & 1.0 \\\\ 0 & -1.5 & -1.25 \\\\ 0 & 0 & 1 \\end{array} \\right] \\left[ \\begin{array}{c} x_1  \\\\ x_2 \\\\ x_3 \\end{array} \\right] =  \\left[ \\begin{array}{c} 0.6931 \\\\ -.460875 \\\\ -.4181 \\end{array} \\right] = c$$\n",
    "\n",
    "Which leaves us with the following equations:\n",
    "$$4x_1+2x_2+x_3=.6931 \\\\ -1.5x_2-1.25x_3=-.460875 \\\\ x_3 = -.4181 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subbing $x_3$ into equation 2 yields $x_2 = \\frac{-.460875 + 1.25(-.4181)}{-1.5} = .6556 $. And thus $x_1 = \\frac{.6931-2.0(.6556)-(-.4181)}{4.0} = -0.05$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, $x = \\left[ \\begin{array}{c} -.05 \\\\ .6556 \\\\  -.4181\\end{array} \\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6931  , -0.460775, -0.4181  ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Quick Check\n",
    "U = np.array([[4.0,2.0,1.0],[0.0,-1.5,-1.25],[0.0,0.0,1.0]])\n",
    "x = np.array([-0.05,0.6556,-.4181])\n",
    "np.dot(U,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Calculate the *residual* $r = b - Ax$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.22044605e-16,  -1.00000000e-04,  -5.00000000e-04])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.array([.6931,1.0986,1.6094])\n",
    "A = np.array([[4.0,2.0,1.0],[9.0,3.0,1.0],[25.0,5.0,1.0]])\n",
    "x = np.array([-0.05,0.6556,-.4181])\n",
    "r = b-np.dot(A,x)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*r* is very close to the zero vector, hence we are correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
